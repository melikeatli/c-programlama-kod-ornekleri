{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Unsupervised Anomaly Detection.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2rqzC4gL1eh"
      },
      "source": [
        "# Usupervised Anomaly Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t9VVTBxL1em"
      },
      "source": [
        "Figure 5 in our post shows the DCGAN architecture. We’ll need to implement our discriminator, generator, data loading, and training code. I usually prefer to start with the the data loading piece - after all, without data we can’t do much! We’ll start by writing a simple script to pull the data from the MVTec website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9vneKh6L1em"
      },
      "source": [
        "### 0. Let's Get Some Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ_ktFXyL2xo",
        "outputId": "78e3862b-3659-4be0-e3f3-0d4c15c8393f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install jupyter\n",
        "!pip install matplotlib\n",
        "!pip install fastai\n",
        "!pip install wget\n",
        "!pip install kornia\n",
        "!pip install opencv-python\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter) (7.16.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.29.5)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter) (7.7.1)\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.10/dist-packages (from jupyter) (4.3.4)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (7.4.9)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.3.3)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (3.0.13)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter) (2.18.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.0.4)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (3.1.4)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.2.5)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.15.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.27.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (75.1.0)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.2.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.10.1)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->jupyterlab->jupyter) (4.12.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.6)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.11.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.16.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.10.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.22.3)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.2.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20241206)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.10/dist-packages (2.7.18)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai) (24.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.7.27)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai) (3.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai) (2.32.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai) (6.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai) (11.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastai) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai) (1.13.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai) (3.7.5)\n",
            "Requirement already satisfied: torch<2.6,>=1.10 in /usr/local/lib/python3.10/dist-packages (from fastai) (2.5.1+cu121)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.6,>=1.10->fastai) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.6,>=1.10->fastai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.6,>=1.10->fastai) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.6,>=1.10->fastai) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.6,>=1.10->fastai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<2.6,>=1.10->fastai) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.17.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4->fastai) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (0.1.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: kornia-rs>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from kornia) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9ZpXj6fL1en"
      },
      "source": [
        "import sys, wget, tarfile, os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "544NPtboL1en"
      },
      "source": [
        "def simple_progress_bar(current, total, width=80):\n",
        "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "    sys.stdout.write(\"\\r\" + progress_message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def get_mvtech_dataset(data_dir, dataset_name):\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    if not (data_dir/('%s.tar.xz'%dataset_name)).exists():\n",
        "        wget.download('ftp://guest:GU.205dldo@ftp.softronics.ch/mvtec_anomaly_detection/%s.tar.xz'%dataset_name, \\\n",
        "                      out=str(data_dir/('%s.tar.xz'%dataset_name)), bar=simple_progress_bar)\n",
        "    if not (data_dir/dataset_name).exists():\n",
        "        tar=tarfile.open(data_dir/('%s.tar.xz'%dataset_name))\n",
        "        tar.extractall(data_dir)\n",
        "        tar.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE5fxZAhL1en"
      },
      "source": [
        "Now, which product class should we experiment with? For some reason, I just really enjoy the aesthetics of the hazelnut class - these images have a cool retro feel to them to me. This of course has nothing to do with how out model will perform - and please feel free to experiment with different classes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LhyyC0wCL1eo",
        "outputId": "5c0007c6-2b8d-45b4-84f5-d4d8062060a0"
      },
      "source": [
        "def get_mvtech_dataset(data_dir, dataset_name):\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    url = f'https://www.mydrive.ch/shares/38886/38300671305b7f7389a1d3869c0f8e2f/download/420546877-1634608126/hazelnut.tar.xz'  # Updated URL to HTTPS\n",
        "\n",
        "    if not (data_dir/('%s.tar.xz'%dataset_name)).exists():\n",
        "        wget.download(url, out=str(data_dir/('%s.tar.xz'%dataset_name)), bar=simple_progress_bar)\n",
        "    if not (data_dir/dataset_name).exists():\n",
        "        tar=tarfile.open(data_dir/('%s.tar.xz'%dataset_name))\n",
        "        tar.extractall(data_dir)\n",
        "        tar.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "URLError",
          "evalue": "<urlopen error 530 Login incorrect.>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror_perm\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'I'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mconnect_ftp\u001b[0;34m(self, user, passwd, host, port, dirs, timeout)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m         return ftpwrapper(user, passwd, host, port, dirs, timeout,\n\u001b[0m\u001b[1;32m   1586\u001b[0m                           persistent=False)\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user, passwd, host, port, dirs, timeout, persistent)\u001b[0m\n\u001b[1;32m   2405\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasswd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m         \u001b[0m_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ftplib.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(self, user, passwd, acct)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendcmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PASS '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ftplib.py\u001b[0m in \u001b[0;36msendcmd\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputcmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ftplib.py\u001b[0m in \u001b[0;36mgetresp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_perm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merror_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror_perm\u001b[0m: 530 Login incorrect.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4098b8ea7efb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hazelnut'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_mvtech_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-e9a69f194faf>\u001b[0m in \u001b[0;36mget_mvtech_dataset\u001b[0;34m(data_dir, dataset_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.tar.xz'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         wget.download('ftp://guest:GU.205dldo@ftp.softronics.ch/mvtec_anomaly_detection/%s.tar.xz'%dataset_name, \\\n\u001b[0m\u001b[1;32m     10\u001b[0m                       out=str(data_dir/('%s.tar.xz'%dataset_name)), bar=simple_progress_bar)\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wget.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mbinurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mulib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0maddinfourl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mftplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error 530 Login incorrect.>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGTV0f5RL1eo"
      },
      "source": [
        "After our script runs, we can have a look at the structure of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K87idb6PL1eo"
      },
      "source": [
        "list((data_path/dset).glob('*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPkyGshL1ep"
      },
      "source": [
        "We're provided a `train`, `test`, and `ground_truth` (labels) folder, and within the train it looks like we're given 391 examples of non-defective hazelnuts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO0QqbcGL1ep"
      },
      "source": [
        "list((data_path/dset/'train').glob('*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1WVVy4kL1eq"
      },
      "source": [
        "im_paths=list((data_path/dset/'train'/'good').glob('*'))\n",
        "im_paths[:5] #Look at first 5 paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erQMELRxL1eq"
      },
      "source": [
        "len(im_paths) #How many examples do we have?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j-xj3QCL1eq"
      },
      "source": [
        "plt.imshow(plt.imread(str(im_paths[0])))\n",
        "plt.title('One sexy hazelnut.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9FADhlvL1eq"
      },
      "source": [
        "Our test folder has a bit more going on, and includes examples of a 4 defect classes, and some more good examples to use in testing. We'll set aside our test set for now, and come back to it after training our GAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTvoJe8FL1er"
      },
      "source": [
        "list((data_path/dset/'test').glob('*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUDCefP4L1er"
      },
      "source": [
        "### 1. Data Loaders for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e51LJkuuL1er"
      },
      "source": [
        "We’ll be using a few python libraries of note here: PyTorch, kornia, and fastai. There’s of course other fantastic tools out there like Tensorflow and Keras - PyTorch and fastai have been my go to for the last 2 years or so, especially for getting stuff up and running quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H50bBpgL1er"
      },
      "source": [
        "import kornia\n",
        "from fastai.vision import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVLZ21FoL1er"
      },
      "source": [
        "# Might need/want to supress warnings if your fastai and pytorch versions dont quite agree\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBZXH7DcL1es"
      },
      "source": [
        "One thing I particularly like about fastai is the built in dataloader classes, called DataBunches. Let's create one for our training set. We'll create a a dataloader that will return minibatches of size 128, downsample our images to 64x64 (the resoultion used in the DCGAN paper). Our `databunch` will also take care of data augmentation and normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUI6IFagL1es"
      },
      "source": [
        "batch_size, im_size, channels = 64, 64, 3\n",
        "tfms = ([*rand_pad(padding=3, size=im_size, mode='border')], [])\n",
        "data = ImageList.from_folder(data_path/dset/'train'/'good').split_none() \\\n",
        "                                                                .label_empty() \\\n",
        "                                                                .transform(tfms, size=im_size) \\\n",
        "                                                                .databunch(bs=batch_size) \\\n",
        "                                                                .normalize((0.5, 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEkQUmaLL1es"
      },
      "source": [
        "data.show_batch(figsize=(8, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXXXanDYL1es"
      },
      "source": [
        "Let's have a quick look at the scale of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FULBYDUaL1es"
      },
      "source": [
        "x, y=data.one_batch()\n",
        "plt.hist(x.numpy().ravel(),100); plt.grid(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B7z8EQfL1et"
      },
      "source": [
        "### 3. Create Models in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjiU9fKBL1et"
      },
      "source": [
        "One thing I love about fastai is that it's a really light wrapper on PyTorch. There's no fastai implemenation of DCGAN, so we have to build it ourselves. Happily, we can do this in PyTorch, and still take advantage of the fastai dataloaders and training code. Before we dive in, let's figure out if we're going to be using a GPU of CPU for training. I highly recommend training this on GPU, CPU is fine for experimentation, but will take quite some time to train the full DCGAN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOK4FnWBL1et"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcHPpTawL1et"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #Do we have a GPU?\n",
        "defaults.device = device\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JikuPo_sL1et"
      },
      "source": [
        "Let's start with setting up our generator. Looking at figure X, we see that our generator is required to upsample between layers - this is generally achieved in Deep Learning models using transposed convolutional layers, also known as fractionally strided convolutional layers. There's a [terrific paper](https://github.com/vdumoulin/conv_arithmetic) on this by Vincent Dumoulin and Francesco Visin.\n",
        "\n",
        "Note that in section 3 of the DCGAN paper, the authors call for batch normalization in the generator and discriminator, and for ReLU activations functions in every layer of the Generator, except for the output layer which uses tanh. We can make our code a bit more succint by create a `conv_trans` method that we can use for each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_1jUB9-L1et"
      },
      "source": [
        "def conv_trans(ni, nf, ks=4, stride=2, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(nf),\n",
        "        nn.ReLU(inplace = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrqmP3V7L1eu"
      },
      "source": [
        "Now we can put there layers together into our generator, following the number of filters shown in figure X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Vi0I0PL1eu"
      },
      "source": [
        "G = nn.Sequential(\n",
        "    conv_trans(100, 1024, ks=4, stride=1, padding=0),\n",
        "    conv_trans(1024, 512),\n",
        "    conv_trans(512, 256),\n",
        "    conv_trans(256, 128),\n",
        "    nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1),\n",
        "    nn.Tanh()).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDDjFQPYL1eu"
      },
      "source": [
        "Alright, so we have a Generator! Now, if you were paying attention in section 3.2 you may remember that the input to our generator during training is just random nosie vectors. And from these random noise vectors, our Generator is supposed to magically produce realisting looking images. Ok, so let's create a random vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOYWmMnL1eu"
      },
      "source": [
        "z = torch.randn(1, 100, 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KB3wvML1ev"
      },
      "source": [
        "And pass it into our Generator and see what we get!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4vXsTo1L1ev"
      },
      "source": [
        "fake = G(z.to(device))\n",
        "plt.imshow(fake[0, 0].cpu().detach().numpy()); plt.grid(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqCpPdTfL1ev"
      },
      "source": [
        "Now, as you can see, our output really just looks like noise! This is of course because we haven't trained our GAN yet! When we're done this output *should* look like a hazelnut!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAT4avWzL1ev"
      },
      "source": [
        "Now let's setup our discriminator. We'll use a similar patter of creating a subfunction that contains our convolution, batch normalization, and ReLU. Note here that the DCGAN authors have called for Leaky ReLU instead of ReLU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPEoY3_1L1ev"
      },
      "source": [
        "def conv(ni, nf, ks=4, stride=2, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(nf),\n",
        "        nn.LeakyReLU(0.2, inplace = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x33GrLjL1ev"
      },
      "source": [
        "D = nn.Sequential(\n",
        "    conv(channels, 128),\n",
        "    conv(128, 256),\n",
        "    conv(256, 512),\n",
        "    conv(512, 1024),\n",
        "    nn.Conv2d(1024, 1, 4, stride=1, padding=0),\n",
        "    Flatten(),\n",
        "    nn.Sigmoid()).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMc1exHfL1ew"
      },
      "source": [
        "Just as a quick sanity check, let's play with our discriminator and generator for a minute. We know that we can pass in random noise vectors into our generator and get crappy fake images out. Now, we should also be able to take these fake images and pass them into our discriminator. Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSGYcQccL1ew"
      },
      "source": [
        "fake=G(z.to(device))\n",
        "fake.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qLLjBEcL1ew"
      },
      "source": [
        "D(fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGZRNjAWL1ew"
      },
      "source": [
        "Just to test our thinking here - what is the meaning of this output value? Well, we create a fake image by passing in noise into our generator, and hav now passed that fake in into our discriminator, which is returning what it belives to the the probably that the image is real. Finally, just one more sanity check. As we train, we'll be passing in both real and fake data into our discriminator. We'll be getting our data from our `fastai` data loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B6iSVx7L1ew"
      },
      "source": [
        "x,y=data.one_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csmc6NUIL1ew"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZbtiTDSL1ex"
      },
      "source": [
        "out=D(x.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Etd3UHL1ex"
      },
      "source": [
        "plt.plot(out.detach().cpu()); plt.grid(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2J1k_TL1ex"
      },
      "source": [
        "So, what are we looking at here? Well, notice that the dimension of our minibatch is `[128, 3, 64, 64]`, meaning we have 128 images we're analyzing at once in our discriminator. The outputs we've plotted are the probabilities of being real the discriminator has assigned to each image. As we can see, our results are all over the place - again, this is becuase we haven't trained anything yet. Once we're done, and effective discriminator should assign a probability close to one to each image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY9Z5b2AL1ex"
      },
      "source": [
        "### 4. Training Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGoxWZPmL1ex"
      },
      "source": [
        "Alright, now that we have our data loader, discriminator, and generator set up, we can train out model! It's really helpful to have some visualization as we train, especially to see if the fake image the generator is creating look convincing. Let's start by creating a performance visualization method to show performance as we train. We'll keep track of a few key visuals while training. First, we'll choose a `z_fized` - 25 randomly chosen and static points in our latent space. At each visualization step, we'll pass these 25 points through our generator, and see how our fake images look. As we train, our random noise should start to be shaped into hazelnuts! Secondly, we'll plot a histogram of the pixel intensity of our fake images `G(z)` and compare these to our histograms of the pixel intensity values in our real images `x`. As we train, these distributions should look more and more similar. Finally, we'll also visualiztion our Generator and Discrimanator loss functions as we train - we should hopefully see a healthy back and forth, if either model consistenly wins, it's unlikely our fake images will look anything like real ones!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTvEPw2oL1ex"
      },
      "source": [
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auw6f8S3L1ey"
      },
      "source": [
        "save_training_viz=True\n",
        "save_dir=Path('data/exports') #Location to save training visualzations\n",
        "(save_dir/'viz').mkdir(exist_ok=True, parents=True)\n",
        "(save_dir/'ckpts').mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JPKEfhmL1ey"
      },
      "source": [
        "def show_progress(save=False):\n",
        "    '''Visualization method to see how were doing'''\n",
        "    plt.clf(); fig=plt.figure(0, (24, 12)); gs=gridspec.GridSpec(6, 12)\n",
        "    with torch.no_grad(): fake=G(z_fixed)\n",
        "    for j in range(30):\n",
        "        fig.add_subplot(gs[(j//6), j%6])\n",
        "        plt.imshow((kornia.tensor_to_image(fake[j])+1)/2); plt.axis('off')\n",
        "    ax=fig.add_subplot(gs[5, :4]); plt.hist(fake.detach().cpu().numpy().ravel(), 100, facecolor='xkcd:crimson')\n",
        "    ax.get_yaxis().set_ticks([]); plt.xlabel('$G(z)$', fontsize=16); plt.xlim([-1, 1])\n",
        "    ax=fig.add_subplot(gs[5, 4:7]); plt.hist(x.cpu().numpy().ravel(), 100, facecolor='xkcd:purple')\n",
        "    ax.get_yaxis().set_ticks([]); plt.xlabel('$x$', fontsize=16)\n",
        "    fig.add_subplot(gs[:,7:])\n",
        "    plt.plot(losses[0], color='xkcd:goldenrod', linewidth=2); plt.plot(losses[1], color='xkcd:sea blue', linewidth=2);\n",
        "    plt.legend(['Discriminator', 'Generator'],loc=1, fontsize=16);\n",
        "    plt.grid(1); plt.title('Epoch = ' + str(epoch), fontsize=16); plt.ylabel('loss', fontsize=16); plt.xlabel('iteration', fontsize=16);\n",
        "    display.clear_output(wait=True); display.display(plt.gcf())\n",
        "    if save: plt.savefig(save_dir/'viz'/(str(count)+'.png'), dpi=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTyAdkesL1ey"
      },
      "source": [
        "Now we'll setup our loss function and optimizers following DCGAN paper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye5knIQ8L1ey"
      },
      "source": [
        "optD = optim.Adam(D.parameters(), lr=1e-4, betas = (0.5, 0.999))\n",
        "optG = optim.Adam(G.parameters(), lr=1e-4, betas = (0.5, 0.999))\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFTE8qeDL1ey"
      },
      "source": [
        "And finally we're ready to train! On tricky thing about GANs is that **no on really knows when you should stop training**. One reasonable, but kinda annoying appraoch is to monitor the appearance of the generated samples while training, that's what we'll do here, while taking periodic snapshots of our weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT4KFU3PL1ey"
      },
      "source": [
        "zero_labels = torch.zeros(batch_size).to(device)\n",
        "ones_labels = torch.ones(batch_size).to(device)\n",
        "losses = [[],[]]\n",
        "epochs, viz_freq, save_freq, count = 10000, 100, 500, 0\n",
        "z_fixed = torch.randn(batch_size, 100, 1, 1).to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (x,y) in enumerate(tqdm(data.train_dl)):\n",
        "        #Train Discriminator\n",
        "        requires_grad(G, False); #Speeds up training a smidge\n",
        "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
        "        l_fake = criterion(D(G(z)).view(-1), zero_labels)\n",
        "        l_real = criterion(D(x).view(-1), ones_labels)\n",
        "        loss = l_fake + l_real\n",
        "        loss.backward(); losses[0].append(loss.item())\n",
        "        optD.step(); G.zero_grad(); D.zero_grad();\n",
        "\n",
        "        #Train Generator\n",
        "        requires_grad(G, True);\n",
        "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
        "        loss = criterion(D(G(z)).view(-1), ones_labels)\n",
        "        loss.backward(); losses[1].append(loss.item())\n",
        "        optG.step(); G.zero_grad(); D.zero_grad();\n",
        "\n",
        "        if i%viz_freq==0: show_progress(save_training_viz)\n",
        "        count+=1\n",
        "    if (epoch+1)%save_freq==0:\n",
        "        torch.save(G, save_dir/'ckpts'/('G_epoch_'+str(epoch)+'.pth'))\n",
        "        torch.save(D, save_dir/'ckpts'/('D_epoch_'+str(epoch)+'.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xabZynLjL1ey"
      },
      "source": [
        "And in case you don't want do sit around waiting, here's a sped up trainig animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUxWNay7L1ez"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"1120\" height=\"630\" src=\"https://www.youtube.com/embed/33wVaI7NUPw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjSkdvC1L1ez"
      },
      "source": [
        "Alright, this blows my mind. The fact that we can do something really hard (make realistic fake images) is insane - it makes no sense. But there we have, not perfect, but pretty nice looking fake images of hazelnuts shaped into random noise! This rounds out the first, and most complex step in implementing the AnoGAN approach. A quick note on GAN training before we move on - it's incredibly finicky. Over the course of this writing, I trained 10-15 GANs, and the reproducability is laughable. This could be improved by using the same random seen each time. As we'll see, our GAN is the critical component that allows us to find anomalies, and in my experimens I found that some of my GANs did significantly better than others. My best results were with a batch size of 128 and 2500 epochs of training, even though this model didn't produce the most realistic fake images! I think this just goes to show that GANs are still very much in the reserach phase, and there's lots of exciting/interesting work to be done to make these models production grade.\n",
        "\n",
        "Now that we can produce reasonably realistic fake images of hazelnuts - it's time to use our GAN to find anomalies!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dyh1pRfL1ez"
      },
      "source": [
        "## 5. A Walk Through Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RnieJ4AL1ez"
      },
      "source": [
        "##Load checkpoints if needed:\n",
        "# My favorite checkpoint, 2500 Epochs, batch size=128\n",
        "G=torch.load('weights/G.pth')\n",
        "D=torch.load('weights/D.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7D1UHHkL1ez"
      },
      "source": [
        "So we now have a generator capable of generating fake images of hazelnuts, and a discriminator that's going its very best to distinguish real from fake hazelnut images. Now what? How do we use these find anomalies?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6RgYfzNL1e0"
      },
      "source": [
        "This is where things get really really interesting. Remeber that comment earlier about the good images representing a manifold in a higher dimensional latent space of all images? We're now going to exploit this phenomenon to find anomalies. The idea is this. Given a new image `x`, we're going to compute the representation the image in our GANs latent space, and if our image is on the manifold of good iamges, we're going to call it good. And if it's too far from our manifold, we'll call it an anomaly. Now, if you've been paying attention, you may be thinking someting like: \"Stephen, our GAN can only compute take images from random noise vectors, we can't go the other way!\". And to this I would say, you are completely correct. Neural nets don't generally go backwards :)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxl-KovHL1e0"
      },
      "source": [
        "So what should we do then? Given a new image, we want to find it's representation in latent space. That is, the 100 dimensional vector that corresponds to that image. Solving this exact problem turns out to basically be impossible, but we can do something close. Let's grab a defective image and discuss the idea used by the AnoGAN authors and others to achiece this feat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygK9ZLsPL1e0"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdqehnLaL1e0"
      },
      "source": [
        "im_path=(data_path/dset/'train'/'good').ls()[1]\n",
        "im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "im=cv2.resize(im, (64,64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBdlCVaTL1e0"
      },
      "source": [
        "plt.imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EYljAsXL1e0"
      },
      "source": [
        "We would like to know if how for from the manifold of good images in our Generator's latent space this image lies. Unfortunately, we can't really compute this directly - but we can do something  close - we can try to find the nearest point in our Generator's latent space - and measure how far away this image is from that point, giving us an idea of how well this image \"fits\" with our good images. We can find this nearest neighbor point using a very similar appraoch to our method for training our network. Specifically, we'll pick a random point in latent space, and using gradient descent, walk through latent space from that point guided by gradient descent. Our loss function is relatively simple, just the l1 loss between our generated image and our \"query\" image. This loss function is captured in equation 3 of the anogan paper:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPDjSrHxL1e0"
      },
      "source": [
        "$$\n",
        "\\mathcal{L}_R(z_\\gamma)=\\sum |x-G(z_{\\gamma})|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3UMrDKbL1e1"
      },
      "source": [
        "Let's give this a shot. We'll pick a random staring point and train for 1000 iterations to reduce the l1 loss between our query image and generated image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ThMHo8L1e1"
      },
      "source": [
        "im_tensor=((kornia.image_to_tensor(im).float()/128)-1).to(device) #Scale image between -1 and +1\n",
        "z=torch.randn(1, 100, 1, 1, requires_grad=True, device=device) #Random starting point in latent space\n",
        "opt=optim.Adam([z], lr=2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdR2t6vHL1e1"
      },
      "source": [
        "losses=[]\n",
        "for i in tqdm(range(1000)):\n",
        "    fake=G(z)\n",
        "    loss=torch.nn.L1Loss()(fake.squeeze(0), im_tensor)\n",
        "    loss.backward(); opt.step()\n",
        "    z.grad.zero_(); G.zero_grad()\n",
        "    losses.append(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_TNDbQ_L1e1"
      },
      "source": [
        "plt.plot(losses); plt.grid(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLd3uawkL1e1"
      },
      "source": [
        "fig=plt.figure(0, (8,4))\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.imshow(im); plt.title('Real Image')\n",
        "\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.imshow((kornia.tensor_to_image(G(z))+1)/2); plt.title('Closest Fake Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOgwrcwXL1e1"
      },
      "source": [
        "So, how do our results look? Eh, ok. Not amazing, right? Ideally we should find the closest good image to our defective image, but our good image looks a little rough, right? We definitely lost some fidelity. The AnoGan authors use one more trick to address this issue. As you can imagine, this \"optimizing backwards accross our generator\" is far from a perfect science - and there's lots of things we can do to improve performance. The AnoGAN authors chose to do something really cool here that uses our discrimanator to help our optimization land on a more realistic nearest good image. Specifically, they choose to add a term to our loss function the measures the distance between our discriminator's representation of our real and fake images. Let's see how this works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTzjds4L1e1"
      },
      "source": [
        "We're going to compare the representation of our real and fake images in the feature space our discrminator. The idea here is that through the training process our discriminator has learned lots of useful features (captured in its hidden layers) to measure the perceptual difference between real and fake images. For this reason, techniques like this are sometime call **perceptual loss**. The AnoGAN authors again use L1 loss, as shown in equation 4 of their paper:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YMTDjUWL1e2"
      },
      "source": [
        "$$\n",
        "\\mathcal{L}_D(z_\\gamma)=\\sum |f(x)-f(G(z_{\\gamma}))|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yubjH9vSL1e2"
      },
      "source": [
        "where $f(\\cdot)$ represents the intermediate layers of the discriminator. Let's implement this and add it to our approach. We'll start by pre-computing the intermediate represnetation of our real image, as this will not change during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0q2inqWL1e2"
      },
      "source": [
        "f={} #Precompute feature values for layers\n",
        "with torch.no_grad():\n",
        "    for i in range(1, (len(D)-2)): f[i]=D[:i](im_tensor.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqOKfQ1WL1e2"
      },
      "source": [
        "Next we'll create a method that computes the discriminator feature loss loss given a fake image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_OXVtWJL1e2"
      },
      "source": [
        "def get_d_loss(f_x, fake, D):\n",
        "    loss_d=nn.L1Loss()(f_x[1], D[:1](fake)) #Get loss value from 1st layer of D\n",
        "    for i in range(2, (len(D)-2)):\n",
        "        loss_d+=nn.L1Loss()(f_x[i], D[:i](fake)) #And remainig layers of D\n",
        "    return loss_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZSt467L1e2"
      },
      "source": [
        "Finally, we need to balance our two loss terms (or reconsruction loss and discrminator loss). The anogan authors capture this in thier equation five:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(z_\\gamma) = (1-\\lambda) \\cdot \\mathcal{L}_R(z_\\gamma) + \\lambda \\cdot \\mathcal{L}_D(z_\\gamma).\n",
        "$$\n",
        "\n",
        "Where $\\mathcal{L}(z_\\gamma)$ is our overall loss function value, and $\\lambda$ conrols the balance between our generator and discriminator loss. Let's put all of this together into a nice method we'll call `walk_latent_space`. Using our new loss function, this method will seek to find the z that minimizes our loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBmdTrYPL1e2"
      },
      "source": [
        "def walk_latent_space(G, D, im_tensor, n_iter=1500, lambd=0.1, lr=2e-2, device='cuda'):\n",
        "    f_x={} #Precompute feature values all layers of D\n",
        "    with torch.no_grad():\n",
        "        for i in range(1, (len(D)-2)): f_x[i]=D[:i](im_tensor.unsqueeze(0))\n",
        "\n",
        "    z=torch.randn(1, 100, 1, 1, requires_grad=True, device=device) #random starting point for walk\n",
        "    opt=optim.Adam([z], lr=lr)\n",
        "\n",
        "    losses=[]\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        fake=G(z) #Get fake image\n",
        "        loss_r=torch.nn.L1Loss()(fake.squeeze(0), im_tensor) #Residual loss\n",
        "        loss_d=get_d_loss(f_x, fake, D) #Discrimintator loss\n",
        "        loss=(1-lambd)*loss_r+lambd*loss_d #Total loss\n",
        "\n",
        "        loss.backward(); opt.step()\n",
        "        z.grad.zero_(); G.zero_grad(); D.zero_grad();\n",
        "        losses.append(loss.item())\n",
        "    return {'z':z, 'loss':loss.item(), 'loss_r':loss_r.item(), 'loss_d':loss_d.item(), 'losses':losses}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQhgiE13L1e2"
      },
      "source": [
        "Let's try out our new `walk_latent_space` on a good image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZZujxJ_L1e3"
      },
      "source": [
        "im_path=(data_path/dset/'train'/'good').ls()[1]\n",
        "im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "im=cv2.resize(im, (64,64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syT1itcLL1e3"
      },
      "source": [
        "im_tensor=((kornia.image_to_tensor(im).float()/128)-1).to(device)\n",
        "res=walk_latent_space(G, D, im_tensor, n_iter=1500, lambd=0.1, lr=2e-2, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZxGLb10L1e3"
      },
      "source": [
        "Let's see how well our optimizer is performing by plotting our loss function value as we take optimization steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKfYi5VvL1e3"
      },
      "source": [
        "plt.plot(res['losses']); plt.grid(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsjFRtZCL1e3"
      },
      "source": [
        "Not bad! We can also have a look at the trade-off between our reconsruction loss and discriminator loss. This will be imporant shortly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGupnFMFL1e3"
      },
      "source": [
        "res['loss'], res['loss_d'], res['loss_r']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6clD1v6L1e3"
      },
      "source": [
        "Finally, let's see how we did! How does our synthetic image compar to our real one?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V8bNRcZL1e5"
      },
      "source": [
        "fig=plt.figure(0, (8,4))\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.imshow(im); plt.title('Real Image')\n",
        "\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.imshow((kornia.tensor_to_image(G(res['z']))+1)/2); plt.title('Closest Fake Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxOO5N-_L1e5"
      },
      "source": [
        "Not bad, right? Adding in our discriminator term really helped keep our image look realistic as we searched the latent space!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7nHpm9WL1e5"
      },
      "source": [
        "### 6. Finding Anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEgFOqwyL1e5"
      },
      "source": [
        "Now, we finally have all the pieces we need to fina anomalies using our GAN. To see how this works, let's grab an image with some anomalies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naqKUpFTL1e5"
      },
      "source": [
        "im_path=Path('data/hazelnut/test/print/016.png')\n",
        "im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "im=cv2.resize(im, (64,64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSIo5Q_iL1e5"
      },
      "source": [
        "plt.imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Ldgh_zL1e5"
      },
      "source": [
        "As you can see, our image has \"print\" anomolies, where a mark has been printed on our poor hazelnut! Now, a big part of the anogan idea here is that anoalous images like this shouldn't really exists in the latent space of our Generator, since we only trained it on good images. So when we try to reconsturct our anamolous image, the reconstruction should fail in a very useful way. Ideally, the reconstruction should yeild \"closest good image\" - and this closest image is going to help us find anomalies! Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_PeGLOrL1e6"
      },
      "source": [
        "im_tensor=((kornia.image_to_tensor(im).float()/128)-1).to(device)\n",
        "res=walk_latent_space(G, D, im_tensor, n_iter=1500, lambd=0.1, lr=2e-2, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZPOUId1L1e6"
      },
      "source": [
        "plt.plot(res['losses']); plt.grid(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anNFh2xtL1e6"
      },
      "source": [
        "Alright, let's have a look at our reconstructed image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdTQ5eUiL1e6"
      },
      "source": [
        "fig=plt.figure(0, (8,4))\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.imshow(im); plt.title('Real Image')\n",
        "\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.imshow((kornia.tensor_to_image(G(res['z']))+1)/2); plt.title('Closest Fake Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWJPRJj0L1e6"
      },
      "source": [
        "And as you can see, our reconstuction isn't great. Happily, this is exactly what we wanted! We shouldn't be able to reconstruct anomolous images. Now, if we can measure how poor our reconsruction is, we can use this metric to measure how anamolous our images are - cool, rigtht? And happily, as we dicussed above, we already have a couple metrics that capture the quality of our reconstruction, our reconsruction loss $\\mathcal{L}_R(z_\\gamma)$ and our discriminator loss $\\mathcal{L}_D(z_\\gamma)$. There idea here is that our anomolous images should have a higher reconsruction and discrimator loss. We explore this idea rigorously below. But first, let's discuss one more neat trick. Not only can we use GANs to identify anomolous images, we can also use GANs to identify anomolous *regions* in images. There idea here is that we can compute a \"residual image\" - the difference between our original and reconstructed image - and areas of the residual images with large values *should* correspond to anomalous regions, since the original and reconstructed image are maxiamlly different in these regions. Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQ5zWpHL1e6"
      },
      "source": [
        "xr=np.abs((im.astype('float')/255)-(kornia.tensor_to_image(G(z))+1)/2) #Residual image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIJnbIREL1e6"
      },
      "source": [
        "plt.imshow(np.mean(xr, axis=2))\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWwsFtPHL1e7"
      },
      "source": [
        "See the anomolous regions glowing? Pretty cool, right? We can take this a step further and threshold our redidual image to identify anomolous pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7CPOl4AL1e7"
      },
      "source": [
        "thresh=0.5\n",
        "plt.imshow(np.mean(xr, axis=2)>thresh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdpPqW6cL1e7"
      },
      "source": [
        "Finally, let's put it all together;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLACnicXL1e7"
      },
      "source": [
        "im_mask=np.zeros_like(im)\n",
        "for i in range(3): im_mask[:,:,i]=np.mean(im, axis=2)\n",
        "im_mask[np.mean(xr, axis=2)>thresh]=im_mask[np.mean(xr, axis=2)>thresh]//2+np.array([255,0,255])//2\n",
        "\n",
        "fig=plt.figure(0, (8,8))\n",
        "fig.add_subplot(2,2,1); plt.imshow(im); plt.title('Image w/ Defect')\n",
        "fig.add_subplot(2,2,2); plt.imshow((kornia.tensor_to_image(G(res['z']))+1)/2); plt.title('Reconstructed Image')\n",
        "fig.add_subplot(2,2,3); plt.imshow(np.mean(xr, axis=2)); plt.title('Residual Image')\n",
        "fig.add_subplot(2,2,4); plt.imshow(im_mask); plt.title('Anomalous Region')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZx_whfzL1e7"
      },
      "source": [
        "Alright, back to that idea of using our reconstruction and discriminator loss to identify anomolous images. Let's loop over all of our test iamges, and for each we'll created a reconstructed image, and compute our reconsruction and discriminator loss for each image. We'll then visualize these results to see if we can use these metrics to find anomolous images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l2w4bXWdL1e7"
      },
      "source": [
        "params={'n_iter':1500, 'lambd':0.1, 'lr':2e-2}\n",
        "all_res=[]\n",
        "im_paths=[p for p in (data_path/dset/'test').glob('*/*.png')]\n",
        "for im_path in im_paths:\n",
        "    im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "    im=cv2.resize(im, (64,64))\n",
        "\n",
        "    im_tensor=((kornia.image_to_tensor(im).float()/128)-1).to(device)\n",
        "    res=walk_latent_space(G, D, im_tensor, **params, device=device)\n",
        "\n",
        "    res['im_fake']=(kornia.tensor_to_image(G(res['z']))+1)/2 #Scaled between 0 and 1\n",
        "    res['im_path']=im_path\n",
        "    res['label']=im_path.parent.name\n",
        "    all_res.append(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynknZWWcL1e8"
      },
      "source": [
        "#Cache results for later if needed\n",
        "# with open(save_dir/'anogan-results-cache-1.p', 'wb') as f:\n",
        "#     pickle.dump(all_res, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7NOJmnjL1e8"
      },
      "source": [
        "# Load previous results if needed\n",
        "# with open(save_dir/'anogan-results-cache-1.p', 'rb') as f:\n",
        "#     all_res = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usQoCVxjL1e8"
      },
      "source": [
        "labels=np.unique([res['label'] for res in all_res])\n",
        "cm={l:i for i,l in enumerate(labels)}\n",
        "\n",
        "my_cmap={0:'r', 1:'b', 2:'g', 3:'c', 4:'m', 5:'y'}\n",
        "my_markers={0:'o', 1:'x', 2:'^', 3:'<', 4:'>', 5:'*'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXM9giPlL1e8"
      },
      "source": [
        "fig=plt.figure(0, (8,8))\n",
        "handles={}\n",
        "for res in all_res:\n",
        "    handles[res['label']]=plt.scatter(res['loss_d'], res['loss_r'], c=my_cmap[cm[res['label']]], marker=my_markers[cm[res['label']]])\n",
        "plt.legend(handles=handles.values(), labels=handles.keys(), fontsize=14); plt.grid(1)\n",
        "plt.xlabel('Discriminator Loss', fontsize=14)\n",
        "plt.ylabel('Residual Loss', fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqdpRieuL1e8"
      },
      "source": [
        "Alright, this plot is really interesting! To be honest it didn't come at quite as cleanly as I expected, but this happens fairly frequently in ML, especially when experimenting with new types of algorithms like GANs. Since our generator should find it more difficult to reproduce anomlous images, we *should* see a higher reconstruction loss for our anomolous images, relative to our good images. As you can see, this isn't really true, with our good images having medium to high reconsruction losses. However, we do see a pretty reasonable/nice pattern with our discriminator - anomalous images tend to have higher discriminator losses. We can use this to create our very own anomalous image detector. We'll create a simple method that takes in an image, computed our reconsruction, and returns true if our discrimnator and reconsruction loss exceed set thresholds. Not you could do something more sophisticated here, such as multivariate logistic regression (or even another neural network!) to classify anomolous images in the space of our discriminator and residual loss - this may lead to overfitting and we would likeley want to introduce a proper test set, but could be interesting to experiment with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfOcAnaEL1e8"
      },
      "source": [
        "def anomaly_detector(im, G, D, d_loss_thresh, r_loss_thresh):\n",
        "    '''Decides if image im is anomalous'''\n",
        "    im_tensor=((kornia.image_to_tensor(im).float()/128)-1).to(device)\n",
        "    res=walk_latent_space(G, D, im_tensor, n_iter=1500, lambd=0.1, lr=2e-2, device='cuda')\n",
        "    if res['loss_d']>d_loss_thresh or res['loss_r']>r_loss_thresh: return True\n",
        "    else: return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nn37IvBL1e8"
      },
      "source": [
        "Ok, let's test our our new function! We'll choose a `d_loss_thresh` of 1.3 (just based on eye balling plot), and a `r_loss_thresh` value of 0.21. Note that our redisual loss threshold isn't really doing much here, but may be useful for other types of images or other GAN weights. Let's measure the accuracy of our anomaly detector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpLyhJtL1e8"
      },
      "source": [
        "im_path=data_path/dset/'test'/'good'/'001.png' #Test on good example - should return False\n",
        "im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "im=cv2.resize(im, (64,64))\n",
        "predicted_anomaly=anomaly_detector(im, G, D, d_loss_thresh=1.3, r_loss_thresh=0.21)\n",
        "print(predicted_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVi10Nt6L1e9"
      },
      "source": [
        "im_path=data_path/dset/'test'/'crack'/'001.png' #Test on defect - should return True\n",
        "im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "im=cv2.resize(im, (64,64))\n",
        "predicted_anomaly=anomaly_detector(im, G, D, d_loss_thresh=1.3, r_loss_thresh=0.21)\n",
        "print(predicted_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjqd3VbHL1e9"
      },
      "source": [
        "Alright, 2 for 2! Now, let's see what our accuracy is accross all our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ACFB81OuL1e9"
      },
      "source": [
        "num_correct=0; total=0 #Measure accuracy on good test examples\n",
        "for im_path in (data_path/dset/'test'/'good').glob('*.png'):\n",
        "    im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "    im=cv2.resize(im, (64,64))\n",
        "    predicted_anomaly=anomaly_detector(im, G, D, d_loss_thresh=1.3, r_loss_thresh=0.21)\n",
        "    if not predicted_anomaly: num_correct+=1\n",
        "    total+=1\n",
        "    print(im_path, predicted_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsTuanDzL1e9"
      },
      "source": [
        "num_correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMsqB1n8L1e9"
      },
      "source": [
        "num_correct=0; total=0 #Measure accuracy on defective test examples\n",
        "for im_path in (data_path/dset/'test').glob('*/*'):\n",
        "    if 'good' in str(im_path): continue\n",
        "    im=cv2.cvtColor(cv2.imread(str(im_path)), cv2.COLOR_BGR2RGB)\n",
        "    im=cv2.resize(im, (64,64))\n",
        "    predicted_anomaly=anomaly_detector(im, G, D, d_loss_thresh=1.3, r_loss_thresh=0.21)\n",
        "    if predicted_anomaly: num_correct+=1\n",
        "    total+=1\n",
        "    print(im_path, predicted_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C9NvLvqL1e-"
      },
      "source": [
        "num_correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0OrWLtDL1e-"
      },
      "source": [
        "For comparison, the MVTec AD paper sites an accruacy on hazelnuts using AnoGAN of 0.83 for the good examples, and 0.16 for the anamolous examples. We appear to be performing a bit better with an accuracy on good examples of around 98% and accuracy on anomalous examples of 68%. There are some differences between decisioning criteria, and our ad-hoc threshold based decisioning may not generalize terribly well, this would require more experimentation - but not a bad start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pUFOzvGL1e-"
      },
      "source": [
        "Finally, if you would like to wrap up (and maybe deploy!) our anomaly detector, check out the `anomaly_detector.py` script, you can run this script from the terminal like this:\n",
        "\n",
        "```\n",
        "python anomaly_detector.py --path_to_image PATH_TO_IMAGE --weights_dir PATH_TO_WEIGHTS --d_loss_thresh=1.3 --r_loss_thresh 0.21\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOYhSuc5L1e-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}